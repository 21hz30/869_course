{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNCMdeLZF5IfMD1l0Ik0bXl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/21hz30/869_course/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "aLwBG6hRM0xQ",
        "outputId": "c93f2c5a-a73d-472a-c62c-6f20d402b040"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bd5acf72176d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLIMB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;31m# We apply our action and observe the outcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0;31m# We print the transition and reward for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State: {}, Action: {}, State': {}, Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m                    \u001b[0mSTATE_TO_TEXT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACTION_TO_TEXT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTATE_TO_TEXT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-bd5acf72176d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# Get the entry for this state-action pair in our transition table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mtransition_entry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# Since we have a low number of fixed states, we can process the entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: None"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(4)\n",
        "\n",
        "# Define our actions\n",
        "CLIMB=0\n",
        "PREPARE=1\n",
        "ACTIONS=[CLIMB,PREPARE]\n",
        "\n",
        "# Define our states\n",
        "BASE_CAMP=0\n",
        "BASE_CAMP_PREPARED=1\n",
        "GLACIER=2\n",
        "GLACIER_PREPARED=3\n",
        "ICE_WALL=4\n",
        "ICE_WALL_PREPARED=5\n",
        "PEAK=6\n",
        "\n",
        "# We define a mapping of actions and states for readability later\n",
        "ACTION_TO_TEXT={CLIMB:'CLIMB', PREPARE:'PREPARE'}\n",
        "STATE_TO_TEXT={BASE_CAMP:'BASE_CAMP', BASE_CAMP_PREPARED: 'BASE_CAMP_PREPARED', GLACIER: 'GLACIER', GLACIER_PREPARED: 'GLACIER_PREPARED', \\\n",
        "               ICE_WALL: 'ICE_WALL', ICE_WALL_PREPARED: 'ICE_WALL_PREPARED', PEAK: 'PEAK'}\n",
        "\n",
        "# An OpenAI Gym code skeleton of the mountain climbing MDP for A2\n",
        "# Developed for MMAI-845\n",
        "class mountainClimbing(gym.Env):\n",
        "    def __init__(self,\n",
        "            ):\n",
        "\n",
        "        # We set the number of states internally\n",
        "        self.num_states = 7\n",
        "\n",
        "        # We must set the size of our observations and actions so an agent\n",
        "        # can be created for the environment\n",
        "        self.observation_space = gym.spaces.Discrete(self.num_states)\n",
        "        self.action_space = gym.spaces.Discrete(len(ACTIONS))\n",
        "        \n",
        "        # Task A.1\n",
        "        # INSERT CODE HERE\n",
        "        # Fill in the correct start state to reset into\n",
        "        self.init_state = BASE_CAMP \n",
        "\n",
        "        # Create an empty table to hold our transition probabilities\n",
        "        self.transition_table = {}\n",
        "        \n",
        "        # Task A.2\n",
        "        # INSERT CODE HERE\n",
        "        # Fill out the correct values for the probabilities for each state (Switch the None values to the right values)\n",
        "        # Remember that each probability must add to 1\n",
        "        # All of the entries for the table must be filled\n",
        "        self.transition_table[BASE_CAMP] = {\n",
        "                CLIMB: {GLACIER: 3, BASE_CAMP: -5},\n",
        "                PREPARE: {BASE_CAMP_PREPARED: -1}, \n",
        "                }\n",
        "\n",
        "        self.transition_table[BASE_CAMP_PREPARED] = {\n",
        "                CLIMB: {GLACIER: 3},\n",
        "                }\n",
        "\n",
        "        self.transition_table[GLACIER] = {\n",
        "                CLIMB: {ICE_WALL: 3, GLACIER: -5},\n",
        "                PREPARE: {GLACIER_PREPARED: -1}, \n",
        "                }\n",
        "\n",
        "        self.transition_table[GLACIER_PREPARED] = {\n",
        "                CLIMB: {ICE_WALL: 3},\n",
        "                }\n",
        "\n",
        "        self.transition_table[ICE_WALL] = {\n",
        "                CLIMB: {PEAK: 3, ICE_WALL: -5},\n",
        "                PREPARE: {ICE_WALL_PREPARED: -1}, \n",
        "                }\n",
        "\n",
        "        self.transition_table[ICE_WALL_PREPARED] = {\n",
        "                CLIMB: {PEAK: 3},\n",
        "                }\n",
        "\n",
        "        # We define a fixed reward table based on the possible transitions\n",
        "        # It is possible to calculate this purely on the transition without\n",
        "        # predefining this table as well\n",
        "        # Task A.3\n",
        "        # INSERT CODE HERE\n",
        "        # Fill out this table with the correct rewards from the MDP for every possible transition (Switch the None values to the right values)\n",
        "        self.reward_table = {\n",
        "                (BASE_CAMP, CLIMB, BASE_CAMP): 0.1,\n",
        "                (BASE_CAMP, CLIMB, GLACIER): 0.9,\n",
        "                (BASE_CAMP, PREPARE, BASE_CAMP_PREPARED): 1,\n",
        "                (BASE_CAMP_PREPARED, CLIMB, GLACIER): 1,\n",
        "                (GLACIER, CLIMB, GLACIER): 0.7,\n",
        "                (GLACIER, CLIMB, ICE_WALL): 0.3,\n",
        "                (GLACIER, PREPARE, GLACIER_PREPARED): 1,\n",
        "                (GLACIER_PREPARED, CLIMB, ICE_WALL): 1,\n",
        "                (ICE_WALL, CLIMB, ICE_WALL): 0.3,\n",
        "                (ICE_WALL, CLIMB, PEAK): 0.7,\n",
        "                (ICE_WALL, PREPARE, ICE_WALL_PREPARED): 1,\n",
        "                (ICE_WALL_PREPARED, CLIMB, PEAK): 1,\n",
        "                }\n",
        "\n",
        "    # Place us in the initial state\n",
        "    # This does not need to be deterministic\n",
        "    # Returns:\n",
        "    #   obs: an observation of our current state after the reset\n",
        "    def reset(self):\n",
        "        self.state = self.init_state\n",
        "        return self._get_obs()\n",
        "\n",
        "    # Get the observation based on our current state\n",
        "    # This function is simple here, but may be more complex depending on\n",
        "    # the task\n",
        "    # Returns:\n",
        "    #   obs: the observation of the current state\n",
        "    def _get_obs(self):\n",
        "        return self.state\n",
        "\n",
        "    # This function progresses the environment one timestep given the current\n",
        "    # state and the action. This is where the dynamics are applied.\n",
        "    # Inputs:\n",
        "    #   action: The desired action to apply to the environment\n",
        "    # Returns:\n",
        "    #   obs: observation of the new state\n",
        "    #   reward: the reward received for the transition\n",
        "    #   done: a variable indicating whether we have terminated or not\n",
        "    #   info: a dictionary data structure containing additional information\n",
        "    #         about the environment we may want to track\n",
        "    def step(self, action):\n",
        "        # Get our current state so we can calculate the reward later\n",
        "        state = self._get_obs()\n",
        "\n",
        "        # We have no additional information to pass back now\n",
        "        info = {}\n",
        "\n",
        "        # Get the entry for this state-action pair in our transition table\n",
        "        transition_entry = self.transition_table[state][action]\n",
        "\n",
        "        # Since we have a low number of fixed states, we can process the entry\n",
        "        # into states and probabilities easily directly. With a more complex\n",
        "        # table, we can iterate over the transitions\n",
        "        possible_states = list(transition_entry.keys())\n",
        "        state_probabilities = list(transition_entry.values())\n",
        "\n",
        "        # We use the numpy library to select the next state according to our\n",
        "        # probability distribution\n",
        "        next_state = np.random.choice(possible_states, p=state_probabilities)\n",
        "        \n",
        "        # Task B.1\n",
        "        # INSERT CODE HERE\n",
        "        # Enter the correct state on which we terminate here\n",
        "        done = True if next_state==-1 else False \n",
        "\n",
        "        # Task B.2\n",
        "        # INSERT CODE HERE\n",
        "        # Call the reward function for the transition correctly\n",
        "        # Hint: use the self. prefix to call functions in the object we are in\n",
        "        reward = 1 \n",
        "\n",
        "        # We make sure to update our current state\n",
        "        self.state = next_state\n",
        "        return self._get_obs(), reward, done, info \n",
        "\n",
        "    # This function calculates the reward for a given_transition\n",
        "    # Inputs:\n",
        "    #   state: The current state\n",
        "    #   action: The action applied\n",
        "    #   next_state: The next state we enter\n",
        "    # Returns:\n",
        "    #   reward: The given reward for the transitions\n",
        "    def _get_reward(self, state, action, next_state):\n",
        "        index = (state, action, next_state)\n",
        "        return self.reward_table[index]\n",
        "\n",
        "# This class will output an action for each state according to the specificatin given in the environment\n",
        "class policy():\n",
        "    # Nothing to do for initialization in this case\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    # This function take the state and returns the correct action\n",
        "    # Inputs:\n",
        "    #   state: an integer representing the state\n",
        "    # Output:\n",
        "    #   action: an integer representing the action specified in the assignmnet text\n",
        "    def __call__(self, state):\n",
        "        action = None\n",
        "        # Q2\n",
        "        # INSERT CODE HERE\n",
        "        # Fill out the correct state in the empty lists in the if statements, and attach the correct actions\n",
        "        # You may add more if/elif statements if you wish, but only two are necessary \n",
        "        if state in []:\n",
        "            action = None\n",
        "        elif state in []:\n",
        "            action = None\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    # NOTE: Set the flag below to 0 if you want to debug only the environment code, and 1 if you want to check on the policy\n",
        "    # Keep in mind that if the value is set to zero, only the climb action is taken, so there may still be an error in the \n",
        "    # prepare actions or states which this would not reveal\n",
        "    debug_flag=0\n",
        "\n",
        "    env = mountainClimbing()\n",
        "    # We need to reset the environment to initialize it\n",
        "    state = env.reset()\n",
        "    if debug_flag==0: \n",
        "        done = False\n",
        "        while not done:\n",
        "            # Always select the climb action in this debug mode \n",
        "            action = CLIMB \n",
        "            # We apply our action and observe the outcome\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            # We print the transition and reward for visualization\n",
        "            print(\"State: {}, Action: {}, State': {}, Reward: {}\".format(\\\n",
        "                    STATE_TO_TEXT[state], ACTION_TO_TEXT[action], STATE_TO_TEXT[next_state], reward))\n",
        "            state = next_state\n",
        "\n",
        "    elif debug_flag==1:\n",
        "        pol = policy()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # Call our policy to get the action for this state\n",
        "            action =  pol(state) \n",
        "            # We apply our action and observe the outcome\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            # We print the transition and reward for visualization\n",
        "            print(\"State: {}, Action: {}, State': {}, Reward: {}\".format(\\\n",
        "                    STATE_TO_TEXT[state], ACTION_TO_TEXT[action], STATE_TO_TEXT[next_state], reward))\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}